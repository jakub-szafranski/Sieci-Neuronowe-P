{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "941d8947",
   "metadata": {},
   "source": [
    "## NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff7083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231890c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(nn.Module):\n",
    "    def __init__(self, input_size: int = 28*28, output_size: int = 10):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.reshape(x.size(0), -1)  # Flatten the input\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25930e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels) \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        identity = x \n",
    "\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        \n",
    "        out = out + self.shortcut(identity)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, n_channels: int = 3, n_classes: int = 10):\n",
    "        super().__init__()\n",
    "        self.initial_conv = nn.Conv2d(n_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 64, num_blocks=2, stride=1)\n",
    "        self.layer2 = self._make_layer(64, 128, num_blocks=2, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, num_blocks=2, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, num_blocks=2, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, n_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels: int, out_channels: int, num_blocks: int, stride: int) -> nn.Sequential:\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(in_channels, out_channels, stride))\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(BasicBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.relu(self.bn(self.initial_conv(x)))\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef0d93",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KDLoss(nn.Module):\n",
    "    def __init__(self, teacher_model: nn.Module, alpha: float = 0.5, temperature: float = 3.0, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher_model\n",
    "        self.teacher.eval()\n",
    "        self.alpha = alpha\n",
    "        self.T = temperature\n",
    "        self.ce = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        self.kl = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    def forward(self, student_logits: torch.Tensor, labels: torch.Tensor, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = self.teacher(inputs)\n",
    "        \n",
    "        loss_ce = self.ce(student_logits, labels)\n",
    "        student_soft = nn.functional.log_softmax(student_logits / self.T, dim=1)\n",
    "        teacher_soft = nn.functional.softmax(teacher_logits / self.T, dim=1)\n",
    "        loss_kl = self.kl(student_soft, teacher_soft) * (self.T ** 2)\n",
    "        \n",
    "        return self.alpha * loss_ce + (1 - self.alpha) * loss_kl\n",
    "    \n",
    "\n",
    "class SoftMSELoss(nn.Module):\n",
    "    def __init__(self, teacher_model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.teacher = teacher_model\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, student_logits: torch.Tensor, labels: torch.Tensor, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = self.teacher(inputs)\n",
    "\n",
    "        student_soft = nn.functional.log_softmax(student_logits, dim=1)\n",
    "        teacher_soft = nn.functional.softmax(teacher_logits, dim=1)\n",
    "        return self.mse(student_soft, teacher_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d2c07",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4b4fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Literal\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import random_split, Dataset, Subset\n",
    "\n",
    "\n",
    "def get_dataset(name: Literal['MNIST', 'CIFAR10']):\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    if name == 'MNIST':\n",
    "        X_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "        X_train, X_val = random_split(X_train, [50000, 10000], generator=torch.Generator().manual_seed(42))\n",
    "        X_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    elif name == 'CIFAR10':\n",
    "        X_train = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "        X_train, X_val = random_split(X_train, [40000, 10000], generator=torch.Generator().manual_seed(42))\n",
    "        X_test = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {name}\")\n",
    "    \n",
    "    return X_train, X_val, X_test\n",
    "\n",
    "def downsample_dataset(dataset: Dataset, class_label: int, fraction: float) -> Dataset:\n",
    "    indices = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx in range(len(dataset)):\n",
    "        _, label = dataset[idx]\n",
    "        indices.append(idx)\n",
    "        labels.append(label)\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    indices = np.array(indices)\n",
    "    \n",
    "    downsample_mask = labels == class_label\n",
    "    keep_indices = indices[~downsample_mask].tolist()\n",
    "    downsample_indices = indices[downsample_mask]\n",
    "    \n",
    "    n_samples = int(len(downsample_indices) * fraction)\n",
    "    print(f\"Downsampling class {class_label} from {len(downsample_indices)} to {n_samples} samples.\")\n",
    "    np.random.seed(42)\n",
    "    sampled_indices = np.random.choice(downsample_indices, size=n_samples, replace=False).tolist()\n",
    "    \n",
    "    final_indices = keep_indices + sampled_indices\n",
    "    \n",
    "    return Subset(dataset, final_indices)\n",
    "\n",
    "\n",
    "def calculate_class_weights(dataset: Dataset, num_classes: int = 10) -> torch.Tensor:\n",
    "    \"\"\"Calculate class weights based on the full dataset. For balanced metrics\"\"\"\n",
    "    class_counts = torch.zeros(num_classes)\n",
    "    \n",
    "    for _, label in dataset:\n",
    "        class_counts[label] += 1\n",
    "    \n",
    "    class_weights = 1.0 / (class_counts + 1e-6)  \n",
    "    class_weights = class_weights / class_weights.sum() * num_classes  \n",
    "    \n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4779e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model: nn.Module, dataset: Dataset, device: torch.device) -> dict:    \n",
    "    model.eval()\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=None\n",
    "    )\n",
    "    \n",
    "    # Macro-averaged metrics\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='macro'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'balanced_accuracy': balanced_acc,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'precision_per_class': precision_per_class,\n",
    "        'recall_per_class': recall_per_class,\n",
    "        'f1_per_class': f1_per_class,\n",
    "        'support_per_class': support,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro\n",
    "    }\n",
    "\n",
    "def pprint_eval(eval_dict: dict):\n",
    "    print(f\"Balanced Accuracy: {eval_dict['balanced_accuracy']:.4f}\")\n",
    "    print(\"Per-class Metrics:\")\n",
    "    for i in range(len(eval_dict['precision_per_class'])):\n",
    "        print(f\" Class {i}: Precision: {eval_dict['precision_per_class'][i]:.4f}, \"\n",
    "              f\"Recall: {eval_dict['recall_per_class'][i]:.4f}, \"\n",
    "              f\"F1-Score: {eval_dict['f1_per_class'][i]:.4f}, \"\n",
    "              f\"Support: {eval_dict['support_per_class'][i]}\")\n",
    "    print(f\"Macro-Averaged Metrics: Precision: {eval_dict['precision_macro']:.4f}, \"\n",
    "          f\"Recall: {eval_dict['recall_macro']:.4f}, \"\n",
    "          f\"F1-Score: {eval_dict['f1_macro']:.4f}\"\n",
    "          )\n",
    "    \n",
    "    # add numbers to confusion matrix\n",
    "    plt.imshow(eval['confusion_matrix'], cmap='Blues')\n",
    "    for i in range(eval['confusion_matrix'].shape[0]):\n",
    "        for j in range(eval['confusion_matrix'].shape[1]):\n",
    "            plt.text(j, i, eval['confusion_matrix'][i, j], ha='center', va='center', color='black')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xticks(ticks=range(10))\n",
    "    plt.yticks(ticks=range(10))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff848c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model: nn.Module, \n",
    "                train_data: torch.utils.data.Dataset, \n",
    "                val_data: torch.utils.data.Dataset,\n",
    "                batch_size: int,\n",
    "                learning_rate: float,\n",
    "                num_epochs: int,\n",
    "                device: torch.device,\n",
    "                criterion: nn.Module = nn.CrossEntropyLoss(),\n",
    "):\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    metrics = {\"val_accuracy\": [], \"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "        for inputs, labels in train_pbar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if isinstance(criterion, KDLoss):\n",
    "                loss = criterion(outputs, labels, inputs)\n",
    "            else:\n",
    "                loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            train_pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        val_criterion = nn.CrossEntropyLoss()\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_pbar:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = val_criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_val += (preds == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = correct_val / len(val_loader.dataset)\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "        metrics[\"train_loss\"].append(epoch_loss)\n",
    "        metrics[\"val_loss\"].append(val_loss)\n",
    "        metrics[\"val_accuracy\"].append(val_accuracy)\n",
    "\n",
    "    return model, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe33705",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383dd5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = FFN(output_size=10).to(device)\n",
    "# teacher_model = ResNet18(n_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04145dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test = get_dataset('MNIST')\n",
    "# X_train, X_val, X_test = get_dataset('CIFAR10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6ec84",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 3\n",
    "\n",
    "teacher_model, metrics = train_model(\n",
    "    model=teacher_model,\n",
    "    train_data=X_train,\n",
    "    val_data=X_val,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = FFN(output_size=10).to(device)\n",
    "# student_model = ResNet18(n_channels=1, n_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89575fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size before downsampling: {len(X_train)}\")\n",
    "X_train_downsampled = downsample_dataset(X_train, class_label=0, fraction=0.001)\n",
    "print(f\"Size after downsampling class: {len(X_train_downsampled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31dc78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weights = calculate_class_weights(X_train_downsampled, num_classes=10)\n",
    "class_weights = None\n",
    "student_criterion = KDLoss(teacher_model=teacher_model, alpha=0.5, temperature=3.0, class_weights=class_weights)\n",
    "\n",
    "student_model, metrics = train_model(\n",
    "    model=student_model,\n",
    "    train_data=X_train_downsampled,\n",
    "    val_data=X_val,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    criterion=student_criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d29ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = evaluate_model(student_model, X_test, device)\n",
    "pprint_eval(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4655fb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5263a4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pytania Badawcze\n",
    "\n",
    "1. Redukcja klas treningowych:\n",
    "   - Jaka jest maksymalna liczba klas, które można usunąć jednocześnie bez degradacji wyników?\n",
    "   - Jak factor redukcji wpływa na jakość modelu?\n",
    "\n",
    "2. Wpływ hiperparametrów Knowledge Distillation:\n",
    "   - Jak parametr alpha wpływa na jakosc modelu?\n",
    "   - Jaki jest optymalny współczynnik temperatury (T)?\n",
    "\n",
    "3. Wpływ architektury:\n",
    "   - Czy różne architektury (FFN vs ResNet) reagują podobnie na knowledge distillation?\n",
    "   - Czy złożoność modelu wpływa na efektywność transferu wiedzy?\n",
    "\n",
    "4. Porównanie funkcji straty:\n",
    "   - Cross Entropy vs KL Divergence + Cross Entropy vs MSE z soft labels\n",
    "   - Zbilansowane metryki vs standardowe (dla imbalanced data)\n",
    "\n",
    "5. Generalizacja metody:\n",
    "   - Czy KD działa dla różnych architektur (FFN → ResNet, ResNet → FFN)?\n",
    "   - Czy zadziała dla różnych zbiorów danych (MNIST vs CIFAR-10)?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9defa124",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
