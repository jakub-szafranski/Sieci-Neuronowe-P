ğŸ§  Knowledge Distillation przy brakujÄ…cych klasach

Projekt z przedmiotu: Sieci Neuronowe (Zima 2025/26)
Autorzy: Weronika Domczewska, Jakub SzafraÅ„ski, Patryk Wawrzacz

ğŸ“Œ Opis projektu

Celem projektu jest zbadanie, czy zjawisko destylacji wiedzy (knowledge distillation) moÅ¼e umoÅ¼liwiÄ‡ nauczenie modelu ucznia rozpoznawania brakujÄ…cej klasy, mimo Å¼e dane treningowe tej klasy nie sÄ… dostÄ™pne.

InspiracjÄ… jest praca â€œSubliminal Learning: Language Models Transmit Behavioral Traits via Hidden Signals in Dataâ€, ktÃ³ra sugeruje, Å¼e modele mogÄ… przekazywaÄ‡ ukryte sygnaÅ‚y pozwalajÄ…ce odtwarzaÄ‡ informacje niewystÄ™pujÄ…ce bezpoÅ›rednio w danych. W naszym projekcie sprawdzamy, czy podobny mechanizm dziaÅ‚a rÃ³wnieÅ¼ w przypadku sieci neuronowych klasyfikujÄ…cych obrazy.

ğŸ¯ Cel badawczy

WytrenowaÄ‡ model nauczyciela na peÅ‚nym zbiorze danych (MNIST, CIFAR-10).

WytrenowaÄ‡ model ucznia na zbiorze pozbawionym jednej z klas, korzystajÄ…c wyÅ‚Ä…cznie z etykiet pochodzÄ…cych od nauczyciela.

SprawdziÄ‡, czy uczeÅ„:

- jest w stanie odtworzyÄ‡ reprezentacjÄ™ brakujÄ…cej klasy,

- potrafi jÄ… poprawnie klasyfikowaÄ‡,

- wykorzystuje ukryte sygnaÅ‚y z procesu destylacji.

Badamy dwa typy destylacji:

- Soft-label distillation â€” uczeÅ„ otrzymuje peÅ‚ny rozkÅ‚ad prawdopodobieÅ„stwa z modelu nauczyciela.

- Hard-label distillation â€” uczeÅ„ dostaje jedynie finalne predykcje nauczyciela.

Zbiory danych

- MNIST

- CIFAR-10

Architektury sieci neuronowych

- FFN 

- CNN (ResNet)

ğŸ› ï¸ Wykorzystywane narzÄ™dzia

TensorFlow
